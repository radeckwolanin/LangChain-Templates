{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Loader\n",
    "This covers how to load PDF documents into the Document format that we use downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (0.0.249)\n",
      "Requirement already satisfied: openai in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (0.27.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (0.0.16)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tqdm in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyPDF\n",
    "Load PDF using `pypdf` into array of documents, where each document contains the page content and metadata with page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (3.14.0)\n",
      "Collecting pymupdf\n",
      "  Obtaining dependency information for pymupdf from https://files.pythonhosted.org/packages/ca/dd/3301dba92880ed2b0f283074320d1d00ba9afe5d98334239b8a1ba519563/PyMuPDF-1.22.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading PyMuPDF-1.22.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: faiss-cpu in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (1.7.4)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in /home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages (from pypdf) (4.7.1)\n",
      "Downloading PyMuPDF-1.22.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.22.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf pymupdf faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The responsible fine-tuning flow\\nHere are the general steps needed to responsibly fine-\\ntune an LLM for alignment, guided at a high  \\nlevel by Meta’s Responsible AI  framework:\\n1. Define content policies & mitigations\\n2. Prepare data  \\n3. Train the model\\n4. Evaluate and improve performance \\nSTEP 1: DEFINE CONTENT POLICIES & MITIGATIONS \\nBased on the intended use and audience for your \\nproduct, a content policy will define what content \\nis allowable and may outline safety limitations on \\nproducing illegal, violent, or harmful content. These \\nlimits should be evaluated in light of the product \\ndomain, as specific sectors and regions may have \\ndifferent laws or standards. Additionally, the needs \\nof specific user communities should be considered as \\nyou design content policies, such as the development \\nof age-appropriate product experiences. Having \\nthese policies in place will dictate the data needed, \\nannotation requirements, and goals for safety  \\nfine-tuning, including the types of mitigation steps \\nthat will be implemented. These policies will be  \\nused for labeling data in later stages when using  \\nRLHF and in additional product layers, such as  \\nmaking enforcement decisions for user inputs  \\nand model outputs.\\n9\\nJ U LY 2023', metadata={'source': 'data/MetaAI - LLM guide with Llama2, fine tuning.pdf', 'page': 10})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/MetaAI - LLM guide with Llama2, fine tuning.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyMuPDF\n",
    "This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The responsible fine-tuning flow\\nHere are the general steps needed to responsibly fine-\\ntune an LLM for alignment, guided at a high  \\nlevel by Meta’s Responsible AI framework:\\n1. Define content policies & mitigations\\n2. Prepare data  \\n3. Train the model\\n4. Evaluate and improve performance \\nSTEP 1: DEFINE CONTENT POLICIES & MITIGATIONS \\nBased on the intended use and audience for your \\nproduct, a content policy will define what content \\nis allowable and may outline safety limitations on \\nproducing illegal, violent, or harmful content. These \\nlimits should be evaluated in light of the product \\ndomain, as specific sectors and regions may have \\ndifferent laws or standards. Additionally, the needs \\nof specific user communities should be considered as \\nyou design content policies, such as the development \\nof age-appropriate product experiences. Having \\nthese policies in place will dictate the data needed, \\nannotation requirements, and goals for safety  \\nfine-tuning, including the types of mitigation steps \\nthat will be implemented. These policies will be  \\nused for labeling data in later stages when using  \\nRLHF and in additional product layers, such as  \\nmaking enforcement decisions for user inputs  \\nand model outputs.\\n9\\nJULY 2023\\n', metadata={'source': 'data/MetaAI - LLM guide with Llama2, fine tuning.pdf', 'file_path': 'data/MetaAI - LLM guide with Llama2, fine tuning.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign 18.4 (Macintosh)', 'producer': 'Adobe PDF Library 17.0', 'creationDate': \"D:20230717181022-07'00'\", 'modDate': \"D:20230717181111-07'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"data/MetaAI - LLM guide with Llama2, fine tuning.pdf\")\n",
    "data = loader.load()\n",
    "data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize FAISS vectorstore and OpenAI embeddings and use similarity search to pull top 2 relavant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: Fine-tune for product \n",
      "Product-specific fine-tuning enables developers to \n",
      "leverage pretrained models or models with some  \n",
      "fine-tuning for a specific task requiring only limited \n",
      "data and resources. Even with initial fine-tuning \n",
      "performed by Meta, developers can further train the \n",
      "model with domai\n",
      "11: will depend on the specific context in which a product \n",
      "is deployed. Developers should also pay attention \n",
      "to how human feedback and annotation of data may \n",
      "further polarize a fine-tuned model with respect \n",
      "to subjective opinions, and take steps to prevent \n",
      "injecting bias in annotation guidelines an\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") # Get API key from environment variable\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"What does it say about fine tunning the model?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: Fine-tune for product \n",
      "Product-specific fine-tuning enables developers to \n",
      "leverage pretrained models or models with some  \n",
      "fine-tuning for a specific task requiring only limited \n",
      "data and resources. Even with initial fine-tuning \n",
      "performed by Meta, developers can further train the \n",
      "model with domain-specific datasets to improve \n",
      "quality on their defined use case. \n",
      "Fine-tuning adapts the model \n",
      "to domain- or application-\n",
      "specific requirements and \n",
      "introduces additional layers  \n",
      "of safety mitigations. Examples of fine-tuning for a pretrained  \n",
      "LLM include:\n",
      "• Text summarization: By using a pretrained \n",
      "language model, the model can be fine-tuned \n",
      "on a dataset that includes pairs of long-form \n",
      "documents and corresponding summaries. This \n",
      "fine-tuned model can then generate concise \n",
      "summaries for new documents.\n",
      "• Question answering: Fine-tuning a language \n",
      "model on a Q&A dataset such as SQuAD \n",
      "(Stanford Question Answering Dataset) allows \n",
      "the model to learn how to answer questions based \n",
      "on a given context paragraph. The fine-tuned \n",
      "model can then be used to answer questions on \n",
      "various topics.\n",
      "• Sentiment analysis: A model can be fine-tuned  \n",
      "on a dataset of labeled text reviews (positive  \n",
      "or negative sentiment) to recognize sentiment \n",
      "and perform analysis to understand user \n",
      "satisfaction. By training the model on this task-\n",
      "specific dataset, it can learn to predict sentiment \n",
      "in text accurately.\n",
      "These examples showcase how fine-tuning an LLM \n",
      "can be used to specialize the model’s capabilities for \n",
      "specific use cases, improving its performance and \n",
      "making it more suitable for specific applications.  \n",
      "The choice of the foundation model and the task-\n",
      "specific dataset plays a crucial role in achieving the \n",
      "desired results.2\n",
      "8\n",
      "J U LY 2023\n",
      "11: will depend on the specific context in which a product \n",
      "is deployed. Developers should also pay attention \n",
      "to how human feedback and annotation of data may \n",
      "further polarize a fine-tuned model with respect \n",
      "to subjective opinions, and take steps to prevent \n",
      "injecting bias in annotation guidelines and to  \n",
      "mitigate the effect of annotators’ bias. Resources  \n",
      "on this topic include:\n",
      "• Don’t Blame the Annotator: Bias Already Starts in \n",
      "the Annotation Instructions\n",
      "• Annotators with Attitudes: How Annotator Beliefs \n",
      "And Identities Bias Toxic Language Detection\n",
      "There are several other risks to consider, such as \n",
      "overfitting, privacy, and security. To mitigate these \n",
      "risks, carefully design the fine-tuning process by \n",
      "curating a high-quality dataset that is representative \n",
      "of your use case, conduct rigorous evaluations, and \n",
      "test your fine-tuned model’s potential use via red \n",
      "teaming (covered in step four - Evaluate and  \n",
      "improve performance ).\n",
      "STEP 3: TRAIN THE MODEL \n",
      "Fine-tuning involves training the model for a limited \n",
      "number of iterations. Once a pretrained model \n",
      "is loaded in the environment for fine-tuning, the \n",
      "training process involves setting up hyperparameters \n",
      "like epochs, batch size, and learning rate. The data \n",
      "are passed through the model, loss is computed, and \n",
      "weights are updated through backpropagation. The THE RESPONSIBLE FINE-TUNING FLOW\n",
      "STEP 2: PREPARE DATA \n",
      "Developing downstream applications of LLMs begins \n",
      "with taking steps to consider the potential limitations, \n",
      "privacy implications, and representativeness of \n",
      "data for a specific use case. Begin by preparing and \n",
      "preprocessing a clean dataset that is representative \n",
      "of the target domain. This involves tokenizing the text, \n",
      "handling special characters, removing unnecessary \n",
      "information, and splitting the dataset into training, \n",
      "validation, and testing sets. This step may also \n",
      "involve ensuring that data are representative of the \n",
      "end users in the deployment context, for instance, by \n",
      "ensuring there are enough examples from relevant \n",
      "languages if you plan to deploy your product in a  \n",
      "non-English speaking market. Representativeness \n",
      "of data is dependent on the use case and should be \n",
      "assessed accordingly. \n",
      "When fine-tuning for a specific use case it can be \n",
      "beneficial to examine training data for biases, such \n",
      "as gender, racial, linguistic, cultural or other biases. \n",
      "Understanding these patterns is important but it may \n",
      "not always be optimal to filter out all problematic \n",
      "content in training data due to the unintended \n",
      "consequences this filtering may have on subsequent \n",
      "performance and safety mitigations, such as prompt \n",
      "engineering. Instead of removing data, focusing on \n",
      "the representativeness of the data can help prevent \n",
      "a fine-tuned model from perpetuating biases in its \n",
      "generated outputs; what is considered representative \n",
      "10\n",
      "J U LY 2023\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    #print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RemoteEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
