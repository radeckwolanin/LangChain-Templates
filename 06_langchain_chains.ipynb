{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. Chains allow us to combine multiple components together to create a single, coherent application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv chromadb langchain openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"service\"],\n",
    "    template=\"What is a good name for a company that provides {service}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. CloudConnect Solutions\n",
      "2. CloudCatalyst\n",
      "3. SaaS-Tech\n",
      "4. CloudMinds\n",
      "5. SimpliCloud Services\n",
      "6. CloudBoost Solutions\n",
      "7. Skyway Solutions\n",
      "8. Nimbus Technologies\n",
      "9. CloudFactory\n",
      "10. StreamTech Solutions\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"SaaS solutions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are multiple variables, you can input them all at once using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cloud Autom8.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"company\", \"service\"],\n",
    "    template=\"What is a good name for a {company} that provides {service}?\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run({\n",
    "    'company': \"AI Startup\",\n",
    "    'service': \"cloud services and automation\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a chat model in an `LLMChain` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SocialEstate Automation\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"What is a good name for a {company} that provides {service}?\",\n",
    "            input_variables=[\"company\", \"service\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0.8)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "print(chain.run({\n",
    "    'company': \"Real Estate Agency\",\n",
    "    'service': \"provides social media automation\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding memory (state)\n",
    "\n",
    "Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next four colors of a rainbow are green, blue, indigo, and violet.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")\n",
    "# -> The first three colors of a rainbow are red, orange, and yellow.\n",
    "conversation.run(\"And the next 4?\")\n",
    "# -> The next four colors of a rainbow are green, blue, indigo, and violet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a chain to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation.save(\"./chains/06_langchain_chains.json\") \n",
    "# ValueError: Saving of memory is not yet supported.\n",
    "\n",
    "chain.save(\"./chains/06_langchain_chains.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading chain from a disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading openai-chat LLM not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m load_chain\n\u001b[0;32m----> 3\u001b[0m chain \u001b[39m=\u001b[39m load_chain(\u001b[39m\"\u001b[39;49m\u001b[39m./chains/06_langchain_chains.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(chain\u001b[39m.\u001b[39mrun({\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcompany\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWindows manufacturer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mservice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msells quality alluminium windows\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     }))\n",
      "File \u001b[0;32m~/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages/langchain/chains/loading.py:530\u001b[0m, in \u001b[0;36mload_chain\u001b[0;34m(path, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m hub_result\n\u001b[1;32m    529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_chain_from_file(path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages/langchain/chains/loading.py:557\u001b[0m, in \u001b[0;36m_load_chain_from_file\u001b[0;34m(file, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    556\u001b[0m \u001b[39m# Load the chain from the config now.\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m load_chain_from_config(config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages/langchain/chains/loading.py:520\u001b[0m, in \u001b[0;36mload_chain_from_config\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mconfig_type\u001b[39m}\u001b[39;00m\u001b[39m chain not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m chain_loader \u001b[39m=\u001b[39m type_to_loader_dict[config_type]\n\u001b[0;32m--> 520\u001b[0m \u001b[39mreturn\u001b[39;00m chain_loader(config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages/langchain/chains/loading.py:40\u001b[0m, in \u001b[0;36m_load_llm_chain\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mllm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[1;32m     39\u001b[0m     llm_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mllm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     llm \u001b[39m=\u001b[39m load_llm_from_config(llm_config)\n\u001b[1;32m     41\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mllm_path\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[1;32m     42\u001b[0m     llm \u001b[39m=\u001b[39m load_llm(config\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mllm_path\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages/langchain/llms/loading.py:19\u001b[0m, in \u001b[0;36mload_llm_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     16\u001b[0m config_type \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m config_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m type_to_cls_dict:\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mconfig_type\u001b[39m}\u001b[39;00m\u001b[39m LLM not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m llm_cls \u001b[39m=\u001b[39m type_to_cls_dict[config_type]\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m llm_cls(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "\u001b[0;31mValueError\u001b[0m: Loading openai-chat LLM not supported"
     ]
    }
   ],
   "source": [
    "from langchain.chains import load_chain\n",
    "\n",
    "# chain = load_chain(\"./chains/06_langchain_chains.json\")\n",
    "# ValueError: Loading openai-chat LLM not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AluWinCo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(chain.run({\n",
    "    'company': \"Windows manufacturer\",\n",
    "    'service': \"sells quality alluminium windows\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional ways of running LLM Chain.\n",
    "\n",
    "`apply` allows you run the chain against a list of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Aluview Windows'}, {'text': 'Tech Haven'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {'company': \"Windows manufacturer\",'service': \"sells quality alluminium windows\"},\n",
    "    {'company': \"Computer store\",'service': \"sells all kinds of electronics\"},\n",
    "]\n",
    "\n",
    "chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate` is similar to apply, except it return an LLMResult instead of string. LLMResult often contains useful generation such as token usages and finish reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='AluWinTech', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='AluWinTech', additional_kwargs={}, example=False))], [ChatGeneration(text='Tech Haven', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='Tech Haven', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 48, 'completion_tokens': 6, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('1badb667-0ff0-445f-8814-0e03c969e982')), RunInfo(run_id=UUID('2cfa9515-c80e-4b15-8307-c68766de21a1'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the outputs\n",
    "\n",
    "\n",
    "By default, LLMChain does not parse the output even if the underlying `prompt` object has an output parser. If you would like to apply that output parser on the LLM output, use `predict_and_parse` instead of `predict` and `apply_and_parse` instead of `apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRed, orange, yellow, green, blue, indigo, and violet.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all the colors in a rainbow\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[], output_parser=output_parser)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/envs/RemoteEnv/lib/python3.8/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With predict_and_parse:\n",
    "llm_chain.predict_and_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RemoteEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
